# FDK-SBB MCP Server - Test Suite

Umfassende Test-Suite f√ºr den SBB FDK MCP Server mit **119 Tests** nach pytest Best Practices.

## Test-Struktur

```
tests/
‚îú‚îÄ‚îÄ unit/                      # @pytest.mark.unit
‚îÇ   ‚îú‚îÄ‚îÄ test_api_client.py    # 15 Tests - API-Client & Retry-Logik
‚îÇ   ‚îú‚îÄ‚îÄ test_cache.py          # 17 Tests - Cache-Management
‚îÇ   ‚îú‚îÄ‚îÄ test_utils.py          # 26 Tests - Helper-Funktionen
‚îÇ   ‚îú‚îÄ‚îÄ test_models.py         # 23 Tests - Pydantic Validierung
‚îÇ   ‚îî‚îÄ‚îÄ test_service.py        # 18 Tests - Business Logic
‚îú‚îÄ‚îÄ integration/               # @pytest.mark.integration
‚îÇ   ‚îî‚îÄ‚îÄ test_server.py         # 20 Tests - MCP Server Tools
‚îú‚îÄ‚îÄ fixtures/                  # Test-Daten
‚îÇ   ‚îî‚îÄ‚îÄ sample_api_responses.json
‚îî‚îÄ‚îÄ conftest.py                # Pytest Config & Fixtures
```

## Test-Ausf√ºhrung

### Alle Tests

```bash
# Alle Tests ausf√ºhren
uv run pytest

# Mit Coverage
uv run pytest --cov=src/fdk_mcp --cov-report=html
```

### Nach Markierung

```bash
# Nur Unit Tests (schnell, isoliert)
uv run pytest -m unit

# Nur Integration Tests
uv run pytest -m integration

# Unit + Integration
uv run pytest -m "unit or integration"

# Nur schnelle Tests (keine slow-markierten)
uv run pytest -m "unit and not slow"
```

### Spezifische Test-Dateien

```bash
# Nur API-Client Tests
uv run pytest tests/unit/test_api_client.py

# Nur Service Tests
uv run pytest tests/unit/test_service.py

# Nur Server Integration Tests
uv run pytest tests/integration/test_server.py
```

### Verbose Mode

```bash
# Detaillierte Ausgabe
uv run pytest -v

# Mit Test-Namen
uv run pytest -v --tb=short
```

## Test-Markierungen (pytest Best Practices)

- `@pytest.mark.unit` - Unit Tests (schnell, isoliert) - **Standard**
- `@pytest.mark.integration` - Integration Tests - **Standard**
- `@pytest.mark.slow` - Langsame Tests (optional)
- `@pytest.mark.e2e` - End-to-End Tests (optional)
- `@pytest.mark.asyncio` - Asynchrone Tests (pytest-asyncio)

## Coverage

### Coverage Report generieren

```bash
# HTML Report
uv run pytest --cov=src/fdk_mcp --cov-report=html

# Terminal Report
uv run pytest --cov=src/fdk_mcp --cov-report=term-missing

# XML Report (f√ºr CI/CD)
uv run pytest --cov=src/fdk_mcp --cov-report=xml
```

### Coverage anzeigen

```bash
# HTML Report √∂ffnen
open htmlcov/index.html  # macOS
start htmlcov/index.html # Windows
```

## Test-Kategorien

### ‚ö° Unit Tests (99 Tests) - `@pytest.mark.unit`

#### test_api_client.py (15 Tests)

- ‚úÖ Erfolgreiche API-Requests
- ‚úÖ Retry-Logik (3 Versuche, exponentieller Backoff)
- ‚úÖ Error-Handling (404, 403, 429, Timeout)
- ‚úÖ Context-Logging (debug/warning)
- ‚úÖ URL-Konstruktion

#### test_cache.py (17 Tests)

- ‚úÖ Verzeichnis-Erstellung
- ‚úÖ Cache-Freshness (24h-Pr√ºfung)
- ‚úÖ Objekt-Speicherung & -Abruf
- ‚úÖ Metadata-Verwaltung
- ‚úÖ Unicode-Handling
- ‚úÖ File-Persistierung

#### test_utils.py (30 Tests)

- ‚úÖ Error-Handling (alle HTTP-Status-Codes)
- ‚úÖ Text-Truncation (25000 Zeichen)
- ‚úÖ ProgressTracker (Statistiken, Speed, ETA)
- ‚úÖ Markdown/JSON-Formatierung
- ‚úÖ Summary-Generierung

#### test_models.py (29 Tests)

- ‚úÖ Pydantic Input-Validierung
- ‚úÖ Min/Max Length-Checks
- ‚úÖ Pattern-Validierung (object_id, language)
- ‚úÖ Field-Validators (Whitespace)
- ‚úÖ Enum-Werte
- ‚úÖ Extra-Fields-Rejection

#### test_service.py (18 Tests)

- ‚úÖ Cache-Refresh-Logik
- ‚úÖ Object-Details (Cache vs API)
- ‚úÖ Filter-Funktionen (Domain/Query)
- ‚úÖ Property-Suche
- ‚úÖ Download-Logik (parallel)
- ‚úÖ Update-Logik (incremental)

### üîó Integration Tests (20 Tests) - `@pytest.mark.integration`

#### test_server.py (20 Tests)

- ‚úÖ sbb_fdk_list_objects (4 Tests)
- ‚úÖ sbb_fdk_get_object (3 Tests)
- ‚úÖ sbb_fdk_search_properties (2 Tests)
- ‚úÖ sbb_fdk_list_domains (1 Test)
- ‚úÖ sbb_fdk_refresh_cache (1 Test)
- ‚úÖ sbb_fdk_get_cache_stats (1 Test)
- ‚úÖ sbb_fdk_download_all_objects (3 Tests)
- ‚úÖ sbb_fdk_update_cache (3 Tests)
- ‚úÖ Error-Handling (2 Tests)
- ‚úÖ Context-Logging (1 Test)

## Makefile Commands

```bash
# Tests ausf√ºhren
make test

# Mit Coverage
make test-coverage

# Alle Checks (lint + typecheck + test)
make check
```

## CI/CD Integration

### GitHub Actions Beispiel

```yaml
- name: Run Tests
  run: |
    uv sync
    uv run pytest -m "unit or integration" --cov=src/fdk_mcp --cov-report=xml

- name: Upload Coverage
  uses: codecov/codecov-action@v3
  with:
    file: ./coverage.xml
```

## Best Practices

1. **Neue Features**: Schreibe Tests BEVOR du Code implementierst (TDD)
2. **Bug Fixes**: Schreibe einen Test, der den Bug reproduziert, dann fixe ihn
3. **Refactoring**: F√ºhre Tests nach jedem Refactoring aus
4. **Coverage**: Ziel ist >90% Coverage f√ºr kritischen Code
5. **Marks**: Markiere Tests korrekt (@pytest.mark.unit/@pytest.mark.integration)

## Troubleshooting

### Tests schlagen fehl

```bash
# Verbose Output f√ºr Details
uv run pytest -vv --tb=long

# Nur fehlgeschlagene Tests
uv run pytest --lf

# Mit pdb Debugger
uv run pytest --pdb
```

### Import-Fehler

```bash
# Dependencies neu installieren
uv sync --force

# Python-Path pr√ºfen
uv run python -c "import sys; print(sys.path)"
```

### Async-Fehler

```bash
# Async-Mode pr√ºfen
uv run pytest --asyncio-mode=auto
```

## Test-Coverage Ziele

| Modul      | Aktuell  | Ziel    |
| ---------- | -------- | ------- |
| api_client | ~95%     | 95%     |
| cache      | ~90%     | 95%     |
| utils      | ~95%     | 95%     |
| models     | ~100%    | 100%    |
| service    | ~85%     | 90%     |
| server     | ~80%     | 85%     |
| **Gesamt** | **~88%** | **90%** |

## N√§chste Schritte

- [ ] E2E Tests gegen echte API (optional)
- [ ] Performance-Tests (Load Testing)
- [ ] Property-Based Testing (Hypothesis)
- [ ] Mutation Testing (mutmut)
